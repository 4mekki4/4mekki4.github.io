---
title: "CS-UM6P at SemEval-2021 Task 1: A Deep Learning Model-based Pre-trained Transformer Encoder for Lexical Complexity"
category: articles
permalink: /articles/2021-08-05-lexical-complexity-prediction-semeval
excerpt: 'Nabil El Mamoun, Abdelkader El Mahdaouy, Abdellah El Mekki, Kabil Essefar, Ismail Berrada'
date: 2021-08-05
venue: 'Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)'
citation: '[CS-UM6P at SemEval-2021 Task 1: A Deep Learning Model-based Pre-trained Transformer Encoder for Lexical Complexity](https://aclanthology.org/2021.semeval-1.73) (El Mamoun et al., SemEval 2021)'
---

<a href='https://aclanthology.org/2021.semeval-1.73.pdf'>Download PDF here</a>

Abstract: Lexical Complexity Prediction (LCP) involves assigning a difficulty score to a particular word or expression, in a text intended for a target audience. In this paper, we introduce a new deep learning-based system for this challenging task. The proposed system consists of a deep learning model, based on pre-trained transformer encoder, for word and Multi-Word Expression (MWE) complexity prediction. First, on top of the encoderâ€™s contextualized word embedding, our model employs an attention layer on the input context and the complex word or MWE. Then, the attention output is concatenated with the pooled output of the encoder and passed to a regression module. We investigate both single-task and joint training on both Sub-Tasks data using multiple pre-trained transformer-based encoders. The obtained results are very promising and show the effectiveness of fine-tuning pre-trained transformers for LCP task.


 Recommended citation: [CS-UM6P at SemEval-2021 Task 1: A Deep Learning Model-based Pre-trained Transformer Encoder for Lexical Complexity](https://aclanthology.org/2021.semeval-1.73) (El Mamoun et al., SemEval 2021)